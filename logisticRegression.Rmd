---
title: "Logistic Regression"
output: 
  html_document:
    toc: true
---
Logistic regression is used when our response variable is qualitative, in particular it is assumed to be bernullian $Y_i \sim Bernoulli(\mu_i)$. This means that the variable can assume the value of 1 or 0 (or True or False).

For the analysis we will use the dataset `Default` from the `ISLR` package [^1]. The dataset that contains customer default records for a credit card company.

```{r, warning=FALSE, message=FALSE}
library(ggplot2)
library(ISLR)
library(tidyverse)
```

Logistic regression models the probability that $Y$ belongs to a particular category. The hypothesis is defined as follow [^2]:

$h_\theta(x) = g(\theta^Tx)$

Where function g is the sigmoid function. The sigmoid function is defined as:

$g(z) = \displaystyle\frac{1}{1+e^{-z}} = \frac{e^z}{1 + e^z}$

```{r sigmoid}
sigmoid <- function(z) {
    g <- 1 / (1 + exp(-z))
    return(g)
}

```

To measure the performance of the model, we use cost functions, these are a measure of how badly a model is predicting our response variable. The logistic regression cost function is:

$J(\theta) = \frac{1}{m} \displaystyle\sum_{i=1}^{m}[-y^{(i)} \log(h_\theta(x^{(i)})) - (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)}))]$

```{r costFunction}
costFunction <- function(theta, X, y) {
    m <- length(y)
    J <- 0
    
    J <- 1/m * ((t(log(sigmoid(X %*% theta))) %*% (-y)) 
            - t(log(1 - sigmoid(X %*% theta))) %*% (1 - y))
    
    return(J)
}
```

The gradient of the cost function is the derivative of it. Gradient is used to minimize our cost function as it represent its slope.

PUT FORMULA

```{r gradient}
gradient <- function(theta, X, y) {
    m <- length(y)
    grad <- rep(0, length(theta))
    
    grad <- 1/m * t(X) %*% (sigmoid(X %*% theta) - y)
    return(grad)
}
```

We upload the data and we convert it into matrix because we are going to use vectorization for our calculation.

```{r loadData}
data(Default)
Default$default <- ifelse(Default$default == "No", 0, 1)
X <- as.matrix(Default[, 3:4])
X <- cbind(matrix(rep(1, nrow(X))), X)
y <- Default[, 1]
ggplot(Default, aes(x = balance, y = income, color = as.factor(default), shape = as.factor(default))) + geom_point()
```

The data shows defaults depending on balance and income. In order to obtain our thetas, we are going to use the built-in optimization function in R: `optim`. As parameters, we provide the initial values of theta (in this case all 0), the cost function to minimize and our gradient. We then select a minimization method (see `?optim` for additional information).

```{r optimalTheta}
initial_theta <- matrix(c(0, 0, 0))
theta_optim <- optim(par = initial_theta, fn = costFunction, gr = gradient, X = X, y = y, method = "BFGS")
```

We are now ready to plot our decision boundary obtained from logistic regression that corresponds to the probability of 0.5.

To calculate slope and intercept we follow the below process:

Our response variable $Y$ is equal to 1 when our hypotesis $h_\theta\geqslant0.5$, that in the case of the sigmoid function, corresponds to $z\geqslant0$. In our case we have:

$\theta_0 + \theta_1 x_1 + \theta_2 x_2 \geqslant 0$

solving per $x_2$ we have:

$x_2\geqslant\displaystyle\frac{-\theta_0}{\theta_2} + \frac{-\theta_1}{\theta_2}x_1$

where our intercept term is: $\displaystyle\frac{-\theta_0}{\theta_2}$

and our slope is: $\displaystyle\frac{-\theta_1}{\theta_2}$

```{r decisionBoundary}
ggplot(Default, aes(x = balance, y = income, color = as.factor(default), shape = as.factor(default))) + geom_point() + geom_abline(slope = (-theta_optim$par[2] / theta_optim$par[3]), intercept = (-theta_optim$par[1] / theta_optim$par[3]), colour = "blue", size = 1)
```

Finally we can perform some predictions. For example, we can calculate the probability that our client with a balance of 2000 and an income of 40000 dollars will not default.

```{r prediction}
sigmoid(c(1, 2000, 40000) %*% theta_optim$par)
```

We are going now to calculate the accuracy on our training setting to 1 all the observations for which the probability is bigger or equal than 0.5.

```{r accuracy}
prediction <- sigmoid(X %*% theta_optim$par) >= 0.5
cat(mean(as.integer(prediction) == y) * 100, "%")
```

<br>

# Polynomial logistic regression with regularization

Regularization allows our algorithms to do not overfit the data and consequently to better perform on the test set.

In order to show how regularization works we start from a dataset containing 2 independent variables, then we calculate the polynomial of these variables until the 6th grade.

The dataset represents whether microchips from a fabrication plant passes quality assurance (QA). We have the test results for some microchips on two different tests. The dataset is the same as the one used for the programming assignment of Andrew Ng's Machine Learning course.

```{r}
# include the download of the dataset.
data <- read.csv("C:\\Git\\StanfordMachineLearning\\machine-learning-ex2\\ex2\\ex2data2.txt", header = F)
colnames(data) <- c("Test1", "Test2", "QAResult")
X <- as.matrix(data)
y <- X[, 3]
X <- X[, 1:2]
ggplot(data, aes(x = Test1, y = Test2, color = as.factor(QAResult), shape = as.factor(QAResult))) + geom_point(size = 2)
```

The regularized logistic regression cost function is given by the following formula:

$J(\theta) = \frac{1}{m} \displaystyle\sum_{i=1}^{m}[-y^{(i)} \log(h_\theta(x^{(i)})) - (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}$

```{r costFunctionReg}
costFunction <- function(theta, X, y, lambda) {
    m <- length(y)
    J <- 0
    
    J <- 1/m * ((t(log(sigmoid(X %*% theta))) %*% (-y)) -
            t(log(1 - sigmoid(X %*% theta))) %*% (1 - y)) +
            lambda / (2 * m) * sum(theta[2:length(theta)] ^ 2)
    
    return(J)
}
```

The gradient is instead given by the following expression. The intercept's parameter, in this case $\theta_0$, is not regularized:

PUT FORMULA

```{r gradientReg}
gradient <- function(theta, X, y, lambda) {
    m <- length(y)
    grad <- rep(0, length(theta))
    
    grad <- 1/m * t(X) %*% (sigmoid(X %*% theta) - y) +
            rbind(0, matrix((lambda/m) * theta[2:length(theta)]))
    
    return(grad)
}
```

With the following function we map the polynomial features. In this case the degree variable is set to 6. This means that, in general, we will calculate the following variables: $x_1^2, x_1x_2,x_2^2,...,x_1^n,x_1^{n-1}x_2,...,x_1x_2^{n-1},x_2^n$

```{r mapFeature}
mapFeature <- function(X1, X2) {
    degree <- 6
    out <- matrix(rep(1, length(X1)))
    for (i in 1:degree) {
        for (j in 0:i) {
            out <- cbind(out, (X1 ^ (i-j)) * (X2 ^ j))
        }
    }
    return(out)
}

```

```{r}
X <- mapFeature(X[, 1], X[, 2])
```

As we did for the non-regularized case, we are going to use the optimization algorithm to minimize our parameters. The difference is that we are now passing also `lambda`.

LAMBDA IS CHOSEN ?????????

```{r}
initial_theta <- matrix(rep(0, ncol(X)))
lambda <- 1
theta_optim <- optim(par = initial_theta, fn = costFunction, gr = gradient, X = X, y = y, lambda = lambda, method = "BFGS")
```

The boundary in this case will not be linear. To plot it we need to map it through the grid...

```{r decisionBoundaryPol}
u <- v <- seq(min(data), max(data), length.out = 100)
z <- matrix(0, nrow =length(u), ncol = length(v))
for (i in 1:length(u)) {
    for (j in 1:length(v)) {
        z[i, j] <- mapFeature(u[i], v[j]) %*% theta_optim$par
    }
}
```

Like the previous case, this is the decision boundary of logistic regression that corresponds to the probability of 0.5

```{r}
rownames(z) <- seq(min(data), max(data), length.out = 100)
colnames(z) <- seq(min(data), max(data), length.out = 100)

as.data.frame(z) %>% 
  rownames_to_column() %>% 
  gather(key, value, -rowname) %>% 
  mutate(key = as.numeric(key), 
         rowname = as.numeric(rowname)) %>%
  ggplot() + geom_contour(aes(x = rowname, y = key, z = value), bins = 1) + 
    geom_point(data=data, aes(x = Test1, y = Test2, color = as.factor(QAResult),
                              shape = as.factor(QAResult)), size = 2)
```

# Multi-class Classification

In this part we are going to implement one-vs-all logistic regression to recognize hand-written digits. The data is the MNIST handwritten digit dataset taken from `keras` package [^3].

```{r}
library(keras)
```

MNIST consists of 28 x 28 grayscale images of handwritten digits. Let's proceed to load the dataset and create variables for our test and training data.

```{r}
mnist <- dataset_mnist()

x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
```

Visualize:
```{r}
par(mfcol=c(6,6))
par(mar=c(0, 0, 3, 0), xaxs='i', yaxs='i')
for (idx in 1:36) { 
    im <- x_train[idx,,]
    im <- t(apply(im, 2, rev)) 
    image(1:28, 1:28, im, col=gray((0:255)/255), 
          xaxt='n', main=paste(y_train[idx]))
}
```

The x data is a 3-d array (images, width, height) of grayscale values. To prepare the data for training we convert the 3-d arrays into matrices by reshaping width and height into a single dimension (28x28 images are flattened into length 784 vectors). Then, we convert the grayscale values from integers ranging between 0 to 255 into floating point values ranging between 0 and 1:

```{r}
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
```

Subset the dataset:

```{r}
x_train <- x_train[1:5000, ]
y_train <- y_train[1:5000]
x_test <- x_test[1:5000, ]
y_test <- y_test[1:5000]
```


Let's define the one-vs-all function, that will return the thetas for our observations optimized:

```{r}
oneVsAll <- function(X, y, num_labels, lambda) {
    
    initial_theta <- matrix(rep(0, ncol(X) + 1))
    X <- cbind(matrix(rep(1, nrow(X))), X)
    y <- matrix(y)
    
    all_theta <- matrix(nrow = num_labels, ncol = ncol(X))
    
    for (i in 0:(num_labels - 1)) {
    theta_optim <- optim(par = initial_theta, fn = costFunction, gr = gradient, X = X, y = matrix(as.numeric(y == i)), lambda = lambda, method = "BFGS")
    
    all_theta[i + 1, ] <- theta_optim$par
    }
    
    return(all_theta)
}
```

We define lambda for regularization and the variable that contains the number of the digits in the dataset.

```{r}
num_labels <- 10
lambda <- 0.1

all_theta <- oneVsAll(x_train, y_train, num_labels, lambda)
```

Once we have the thetas we can perform our prediction and check the performance of our classifier on the test set:

```{r}
X <- cbind(matrix(rep(1, nrow(x_test))), x_test)
prediction <- apply(X %*% t(all_theta), 1, which.max) - 1
cat(mean(as.integer(prediction) == y_test) * 100, "%")
```

[^1]: James, Witten, Hastie, Tibshirani. *An Introduction to Statistical Learning.* New York: Springer-Verlag, 2013. <http://www-bcf.usc.edu/~gareth/ISL/index.html>
[^2]: For formula and notation please see: Ng, Andrew. *Machine Learning*. Stanford University. Online course provided by Coursera. <https://www.coursera.org/learn/machine-learning>. 
[^3]: Keras website shows also how to load and manpulate the package, for completeness, we are reporting some of the information here.
<https://keras.rstudio.com/>